{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLzMvxZpogwE"
      },
      "outputs": [],
      "source": [
        "# boosting---> boosting is a popular technique that is used in ensemble learning which combines multiple weak or base learners to create a stronger pridictive model . the\n",
        "# main idea behind boosting is to sequentially train a series of models where each subsequential model focus on the instance that were miss classified by the previous models\n",
        "# this itrative process allows the ensemble to learn from mistakes and improve its overall performmance.\n",
        "# working process of boosting-->(1)initialization--> each instance in the training set is assined in a equal weight (2).itrative training--> a base learner is trained on the\n",
        "# weigheted training set and its predictions are eveluated (3) weight update--> the weight of miss classified instances are increased whihe correctly classifed instances receved\n",
        "# lower weights (4)ensemble creation--> the train base learners is added to the enseble with a weight that depends on its performance (5)itration termination--> the process\n",
        "# continuous for a predefine numbers of itration or until a performance (6)final prediction--> the ensemble combines the predictions of all base learners typically uses\n",
        "# a weigheted voted scheme\n",
        "# types of boosting-->(1)adaboost_--> it assignes heigher weighets to missclassified instances and focus on those instances during subtitration titrations its sequentially trains a series\n",
        "# of weak learners and combines there predictions to the form of the final example adaboost is primaly used for binary classification problems\n",
        "# (2)--> gradient boosting--> it builts an ensemble of weak learners in a stage wise maners each subsequent model is trained to correct them stage made by the previous models\n",
        "# by fitting the negative gradient of the loose function gradient boosting can handle both classifiation and regression task and it often used with decision trees as a\n",
        "# base learner.((1))extent gradient boosting(xgboost)--> it is a optimized implementian on gradient boosting that offers several inhasments including parellel processing\n",
        "# regularization technique and handling missing values .it usea a combination of tree base models and linear models for boosting which allows it to capture both linear\n",
        "# and non linear regression sheets in the data efficiency((2))light gbm--> it focused on achiving faster training speed and lower momery uses it uses a novel tree glory algorithm\n",
        "# called gradient based one side sampling to select the most informative instances for building decesion trees((3))cat boost--> cat boost is a gradient boosting algorithm that is\n",
        "# designed to handel categorical features directly without the need for extencive data preprocessing it incorporates an inovative method to handel categorical variables\n",
        "# which includes applying a combination of ordered boostiing and symmtric trees((4))Stochastic --> Stochastic gradind boosting introdce randomnes into the boosting process by\n",
        "# subsempling the traininng data for features at each itrations it helps to reduce overfitting and can improve the models generalization ability specially when dealing with\n",
        "# large datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjOheFaHr3gE",
        "outputId": "5c003930-1834-4656-d643-9c56fe9663f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n"
          ]
        }
      ],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_YkIqpHwA_y",
        "outputId": "a14ac64a-c4fc-4a44-c58f-305e076421c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adaboost Accuracy: 0.85\n",
            "XGBoost Accuracy: 0.895\n"
          ]
        }
      ],
      "source": [
        "# .importing necessary liabreries\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generating a synthetic classification dataset\n",
        "x,y=make_classification(n_samples=1000,n_features=10,random_state=42)\n",
        "\n",
        "# splitting the dataset into taining and testing sets\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
        "\n",
        "\n",
        "# creating and training an adaboost classifier\n",
        "adaboost=AdaBoostClassifier(n_estimators=100,random_state=42)\n",
        "adaboost.fit(x_train,y_train)\n",
        "\n",
        "# # creeating and training on xgboost classifier\n",
        "xgboost=XGBClassifier(n_estimators=100,random_state=42)\n",
        "xgboost.fit(x_train,y_train)\n",
        "\n",
        "# making predictions on the test set for adaboost]\n",
        "y_pred_adaboost=adaboost.predict(x_test)\n",
        "\n",
        "# # making predictions ont he rest for xgboost\n",
        "y_pred_xgboost=xgboost.predict(x_test)\n",
        "\n",
        "# calculating accuracy for adaboost and xgboost\n",
        "adaboost_accuracy = accuracy_score(y_test,y_pred_adaboost)\n",
        "xgboost_accuracy = accuracy_score(y_test,y_pred_xgboost)\n",
        "\n",
        "print(\"Adaboost Accuracy:\", adaboost_accuracy)\n",
        "print(\"XGBoost Accuracy:\", xgboost_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4atMw30Hxb41",
        "outputId": "284fbe2e-5ab5-4596-92bc-7095ba953479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWVV9LGZzUeN",
        "outputId": "1fa1f587-dd75-49b2-de8c-61609bf4db58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.2)\n"
          ]
        }
      ],
      "source": [
        "pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhXbS2YwzXRf"
      },
      "outputs": [],
      "source": [
        "# import necesssary liabraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVOcSxJS1p6N"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uARHWZuz6q0",
        "outputId": "57e3447c-9bc2-4245-c51d-7914eee67f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting classifier Accuracy: 0.9\n",
            "[LightGBM] [Info] Number of positive: 388, number of negative: 412\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2550\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485000 -> initscore=-0.060018\n",
            "[LightGBM] [Info] Start training from score -0.060018\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightGBM Classifier Accuracy: 0.88\n",
            "CatBoost Classifier Accuracy: 0.885\n",
            "Stochastic Gradient Boosting Classifier Accuracy: 0.88\n",
            "Gradient Boosting Regressor Accuracy: 0.6542269905134531\n"
          ]
        }
      ],
      "source": [
        "# gradient boosting classifier\n",
        "gb_classifier=GradientBoostingClassifier(n_estimators=100,random_state=42)\n",
        "gb_classifier.fit(x_train,y_train)\n",
        "y_pred_gb=gb_classifier.predict(x_test)\n",
        "accurecy_gb=accuracy_score(y_test,y_pred_gb)\n",
        "print(\"Gradient Boosting classifier Accuracy:\",accurecy_gb)\n",
        "\n",
        "# light GBM Classifier\n",
        "lgb_classifier=LGBMClassifier(n_estimators=100,random_state=42)\n",
        "lgb_classifier.fit(x_train,y_train)\n",
        "y_pred_lgb=lgb_classifier.predict(x_test)\n",
        "accuracy_lgb=accuracy_score(y_test,y_pred_lgb)\n",
        "print(\"LightGBM Classifier Accuracy:\",accuracy_lgb)\n",
        "\n",
        "# CatBoost Classifier\n",
        "cat_classifier=CatBoostClassifier(n_estimators=100,random_state=42,verbose=0)\n",
        "cat_classifier.fit(x_train,y_train)\n",
        "y_pred_cat=cat_classifier.predict(x_test)\n",
        "accuracy_cat=accuracy_score(y_test,y_pred_cat)\n",
        "print(\"CatBoost Classifier Accuracy:\",accuracy_cat)\n",
        "\n",
        "# Stochastic Gradient Boosting Classifier\n",
        "stoch_gb_classifier=HistGradientBoostingClassifier(random_state=42)\n",
        "stoch_gb_classifier.fit(x_train,y_train)\n",
        "y_pred_stoch_gb=stoch_gb_classifier.predict(x_test)\n",
        "accuracy_stoch_gb=accuracy_score(y_test,y_pred_stoch_gb)\n",
        "print(\"Stochastic Gradient Boosting Classifier Accuracy:\",accuracy_stoch_gb)\n",
        "\n",
        "# Gradient Boosting Regressor (for demonstration purpose)\n",
        "gb_regressor=GradientBoostingRegressor(n_estimators=100,random_state=42)\n",
        "gb_regressor.fit(x_train,y_train)\n",
        "y_pred_gb_regressor=gb_regressor.predict(x_test)\n",
        "accurecy_gb_regressor=r2_score(y_test,y_pred_gb_regressor)\n",
        "print(\"Gradient Boosting Regressor Accuracy:\",accurecy_gb_regressor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktrc1s010oLE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
